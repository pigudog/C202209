{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 定义超参数\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 5 # 训练次数\n",
    "# 判断GPU是否可用\n",
    "use_gpu = torch.cuda.is_available()\n",
    "use_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面显示的是我们没有安装anaconda\n",
    "我们进行安装后：pytorch 的数据读取主要包括三个类：1.Dataset,DataLoader,DataLoaderIter,这个三者的大致关系是依次封装的关系：1被装进2，2被装进3\n",
    "\n",
    "DataLoader是一个比较重要的类，它为我们提供的常用操作有：batch_size(每个batch的大小), shuffle(是否进行shuffle操作), num_workers(加载数据的时候使用几个子进程)。\n",
    "\n",
    "Dataset、Dataloader和DataLoaderIter是层层封装的关系，最终在内部使用DataLoaderIter进行迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载训练集 MNIST 手写数字训练集\n",
    "# 数据是datasets类型的\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='../datasets', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='../datasets', train=False, transform=transforms.ToTensor())\n",
    "#　将数据处理成 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # 选择打乱数据\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # 选择不打乱数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ../datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ../datasets\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1f3ac50abc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1f3ad1ecb80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的网络构建类模板\n",
    "class net_name(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_name, self).__init__()\n",
    "        # 可以添加各种网络层\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        # 具体每种层的参数可以去查看文档\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 定义向前传播\n",
    "        out = self.conv1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module 是所有神经网络单元（neural network modules）的基类。pytorch在nn.Module中，实现了__call__方法，而在__call__方法中调用了forward函数。 所以forward函数中依次调用添加到self._module中的子模块，最后输出经过所有神经网络层的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义简单的前馈神经网络，使用ReLU激活函数\n",
    "class neuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
    "        super(neuralNetwork, self).__init__() # super() 函数是用于调用父类(超类)的一个方法\n",
    "# Sequential()表示将一个有序的模块写在一起，也就相当于将神经网络的层按顺序放在一起，这样可以方便结构显示\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, n_hidden_1),\n",
    "            nn.ReLU(True)) # 表示使用ReLU激活函数\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_1, n_hidden_2),\n",
    "            nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_2, out_dim),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "# 定义向前传播\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "# 图片大小是28*28，中间定义了两个隐藏层大小分别为300和100，最后输出层为10，10分类问题\n",
    "model = neuralNetwork(28 * 28, 300, 100, 10)\n",
    "if use_gpu:\n",
    "    model = model.cuda() # 现在可以在GPU上跑代码了\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 定义损失函数类型，使用交叉熵\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # 定义优化器，使用随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个循环表示每个epoch，接着开始前向传播，然后计算loss，然后反向传播，接着优化参数，特别注意的是在每次反向传播的时候需要将参数的梯度归零："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "epoch 1\n",
      "[1/5] Loss: 2.239225, Acc: 0.224531\n",
      "[1/5] Loss: 2.065068, Acc: 0.330000\n",
      "[1/5] Loss: 1.900452, Acc: 0.401007\n",
      "Finish 1 epoch, Loss: 1.881824, Acc: 0.407882\n",
      "**********\n",
      "epoch 2\n",
      "[2/5] Loss: 1.391377, Acc: 0.556042\n",
      "[2/5] Loss: 1.343618, Acc: 0.564271\n",
      "[2/5] Loss: 1.316239, Acc: 0.566771\n",
      "Finish 2 epoch, Loss: 1.313842, Acc: 0.567098\n",
      "**********\n",
      "epoch 3\n",
      "[3/5] Loss: 1.223335, Acc: 0.581667\n",
      "[3/5] Loss: 1.210189, Acc: 0.583125\n",
      "[3/5] Loss: 1.205204, Acc: 0.582847\n",
      "Finish 3 epoch, Loss: 1.203047, Acc: 0.583572\n",
      "**********\n",
      "epoch 4\n",
      "[4/5] Loss: 1.175708, Acc: 0.586094\n",
      "[4/5] Loss: 1.161493, Acc: 0.591979\n",
      "[4/5] Loss: 1.152217, Acc: 0.594392\n",
      "Finish 4 epoch, Loss: 1.151495, Acc: 0.594550\n",
      "**********\n",
      "epoch 5\n",
      "[5/5] Loss: 1.125202, Acc: 0.600521\n",
      "[5/5] Loss: 1.124900, Acc: 0.598307\n",
      "[5/5] Loss: 1.122433, Acc: 0.598941\n",
      "Finish 5 epoch, Loss: 1.121059, Acc: 0.599464\n"
     ]
    }
   ],
   "source": [
    "# 开始模型训练\n",
    "for epoch in range(num_epochs):\n",
    "    print('*' * 10)\n",
    "    print(f'epoch {epoch+1}')\n",
    "    running_loss = 0.0 # 初始值\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(train_loader, 1): # 枚举函数enumerate返回下标和值\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1) # 将图片展开为28*28\n",
    "        # 使用GPU？\n",
    "        if use_gpu:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        # 向前传播\n",
    "        out = model(img) # 前向传播\n",
    "        loss = criterion(out, label) # 计算loss\n",
    "        running_loss += loss.item() # loss求和\n",
    "        _, pred = torch.max(out, 1)\n",
    "        running_acc += (pred == label).float().mean()\n",
    "        # 向后传播\n",
    "        optimizer.zero_grad() # 梯度归零\n",
    "        loss.backward() # 后向传播\n",
    "        optimizer.step() # 更新参数\n",
    "\n",
    "        if i % 300 == 0:\n",
    "            print(f'[{epoch+1}/{num_epochs}] Loss: {running_loss/i:.6f}, Acc: {running_acc/i:.6f}')\n",
    "    print(f'Finish {epoch+1} epoch, Loss: {running_loss/i:.6f}, Acc: {running_acc/i:.6f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.137009, Acc: 0.591162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    ## 模型测试\n",
    "    model.eval() # 让模型变成测试模式,这主要是对dropout和batch\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        if use_gpu:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        with torch.no_grad():\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "        eval_loss += loss.item()\n",
    "        _, pred = torch.max(out, 1)\n",
    "        eval_acc += (pred == label).float().mean()\n",
    "    print(f'Test Loss: {eval_loss/len(test_loader):.6f}, Acc: {eval_acc/len(test_loader):.6f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在pytorch中的view()函数就是用来改变tensor的形状的，例如将2行3列的tensor变为1行6列， view( )相当于numpy中resize（）的功能，但是用法可能不太一样。\n",
    "参数中的-1就代表这个位置由其他位置的数字来推断，比如a tensor的数据个数是6个，如果view（1，-1），我们就可以根据tensor的元素个数推断出-1代表6。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), './neural_network.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e12f081f3b69c47b407a411e35ba4814a2bf694e6a041dab3adb1673ea66113c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
